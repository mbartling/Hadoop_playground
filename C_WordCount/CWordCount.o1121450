##### BEGIN JOB FILE ###################################################
#!/bin/bash
#
# Sample script for running Hadoop on eos.  
#
# This currently operates in two modes:
# 
# 1) interactive mode (INTERACTIVE=1) - start a vncserver with
#    a Firefox session pointing at some useful Hadoop urls.
#    Added to that is an xterm window for spawning new Hadoop jobs.
#    Finally, a xterm window which, when closed, will terminate the  
#    interactive session and stop Hadoop is started
#    (after running the examples below).
#
# 2) batch mode (INTERACTIVE=0) - if no vncserver command available or 
#    no VNC password set, then start HDFS/Hadoop, copy files to HDFS, 
#    run Hadoop app (e.g. MapReduce), copy files from HDFS, 
#    stop Hadoop/HDFS. 
#
#      IMPORTANT NOTE: ONLY ONE HADOOP JOB CAN BE ACTIVE AT A TIME!!!
#
#    If you run more than one Hadoop job at a time you can expect to 
#    have problems with at least one of the jobs.
#
# 
# PBS settings
#
#  - nodes must be >= 2 (2 is best for interactive, larger jobs need batch mode)
#  - ppn must be >= 8
#  - mem should be == nodes X 20gb
#  - walltime should be adjusted as needed
#  - Hadoop requires the use of -W x=NACCESSPOLICY:SINGLEJOB

#PBS -l nodes=2:ppn=8,mem=40gb,walltime=1:00:00
#PBS -W x=NACCESSPOLICY:SINGLEJOB
#PBS -N CWordCount
#PBS -S /bin/bash
#PBS -m abe
#PBS -j oe
#PBS -M michael.bartling15+PBS@gmail.com

# module load python
export INTERACTIVE=0  # 0 = batch mode ; 1 = interactive VNC session

echo "##### BEGIN JOB FILE ###################################################"
cat $0 # remember the job run
echo "##### END JOB FILE #####################################################"
echo -e "\n\n\n\n\n" 
echo "##### BEGIN RUNNING JOB ################################################"
cd $PBS_O_WORKDIR # change directory to where we started
# output the current directory so it is clear where it ran from
echo "The current directory is `pwd`"
## cp the geninputs to the scratch drive generate a lot of inputs and put them on root
#cp ./src/geninputs $SCRATCH/geninputs
#cd $SCRATCH
#mkdir -p inputs
#./geninputs 10000000 >> ./inputs/file01
#
#cd $PBS_O_WORKDIR # change directory to where we started

. /g/software/hadoop/tamusc/conf/hadoop-env.sh # set up environment (to be moved to module file)

echo "##### Initialize Hadoop configuratin in $HADOOP_CONF_DIR"
init-hadoop-conf.sh

echo "##### Start Cluster"
start-hadoop-cluster.sh 

if [ $INTERACTIVE -eq 1 ] ; then # start an interactive VNC session
  start-hadoop-vnc.sh
  if [ $? -ne 0 ] ; then
    echo "Failed to start interactive VNC session... falling back to batch mode."
    let INTERACTIVE=0
  fi
fi

# get HDFS URL (needed to copy files to/from HDFS)
HDFS_URL="`grep hdfs: $HADOOP_CONF_DIR/core-site.xml | cut -f 2 -d '>' | cut -f 1 -d '<'`/"

# example of copying to HDFS
echo "##### Copying files to $HDFS_URL ($TAMUSC_USER_WORK_DIR)"
# use $HDFS_URL as a reference to copy files to hadoop file system
hadoop fs -put $0 $HDFS_URL # copy job file to HDFS root
#hadoop fs -put $0 / # this would accomplish the same thing

# example of running Hadoop (replace these with your Hadoop job(s))
# (from Fedora: https://fedoraproject.org/wiki/Changes/Hadoop#How_To_Test )
echo "##### Running MapReduce examples"
#MREXAMPLES=$TAMUSC_HADOOP_HOME/$TAMUSC_HADOOP_VERSION/hadoop-examples-$TAMUSC_HADOOP_VERSION.jar 
# output the current directory so it is clear where it ran from
echo "The current directory is `pwd`"
HADOOP_STREAMING=$TAMUSC_HADOOP_HOME/$TAMUSC_HADOOP_VERSION/contrib/streaming/hadoop-*streaming-*.jar
#CMINMAX= contrib/streaming/hadoop-*streaming-*.jar
#/g/software/hadoop/2.2.0/share/hadoop/tools/lib/hadoop-streaming-2.2.0.jar
#$TAMUSC_HADOOP_HOME/$TAMUSC_HADOOP_VERSION/share/hadoop/tools/lib/hadoop-*streaming-*.jar

#/g/software/hadoop/2.20.0
#hadoop jar /usr/share/hadoop/mapreduce/hadoop-*streaming-*.jar \
#hadoop jar $CMINMAX
#	-input inputs \
#	-output outputs \
#	-mapper src/myMap \
#	-combiner src/myCollect \
#	-reducer src/myReduce 
echo $TAMUSC_USER_WORK_DIR
echo "##### Generating MinMax inputs"
hadoop dfs -mkdir $TAMUSC_USER_WORK_DIR/inputs
# TODO
# cd $SCRATCH
cd $PBS_O_WORKDIR # change directory to where we started
#hadoop fs -put ./inputs/file01 $HDFS_URL/inputs/.
hadoop fs -copyFromLocal ./inputs/*.* $TAMUSC_USER_WORK_DIR/inputs

echo "##### Running C Min Max"
# Note, the  -file command lets us package the executables and send 
# them to the hadoop cluster (can also do this with files

hadoop jar $HADOOP_STREAMING \
	-mapper map.out \
	-reducer reduce.out \
    -file map.out \
    -file reduce.out \
    -input $TAMUSC_USER_WORK_DIR/inputs/ \
	-output $TAMUSC_USER_WORK_DIR/outputs/

echo "##### Finished running MapReduce job"

# example of examining files on HDFS
echo "##### Files on $HDFS_URL"
hadoop fs -lsr $HDFS_URL
# hadoop fs -lsr / # this would accomplish the same thing

# example of copying files off of HDFS
echo "##### Copying files from $HDFS_URL ($TAMUSC_USER_WORK_DIR) to $PBS_O_WORKDIR"
mkdir -p outputs
hadoop fs -get /`basename $0` $PBS_O_WORKDIR/outputs # get job script we copied earlier
hadoop fs -get $TAMUSC_USER_WORK_DIR/outputs $PBS_O_WORKDIR/outputs
if [ $INTERACTIVE -eq 1 ] ; then # pause here until user ends VNC
  hold-hadoop-vnc-here.sh
fi

echo "##### Stop Cluster"
stop-hadoop-cluster.sh

echo "##### END RUNNING JOB ##################################################"
# EOF
##### END JOB FILE #####################################################






##### BEGIN RUNNING JOB ################################################
The current directory is /g/home/bart4128/Hadoop_playground/C_WordCount
##### Initialize Hadoop configuratin in /g/home/bart4128/.hadoop/conf
##### Start Cluster
Formatting hdfs://node011:9000 (/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu)
14/02/24 17:48:38 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = node011/172.25.1.11
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1503152; compiled by 'mattf' on Mon Jul 22 15:23:09 PDT 2013
STARTUP_MSG:   java = 1.6.0_33
************************************************************/
14/02/24 17:48:39 INFO util.GSet: Computing capacity for map BlocksMap
14/02/24 17:48:39 INFO util.GSet: VM type       = 64-bit
14/02/24 17:48:39 INFO util.GSet: 2.0% max memory = 5592449024
14/02/24 17:48:39 INFO util.GSet: capacity      = 2^24 = 16777216 entries
14/02/24 17:48:39 INFO util.GSet: recommended=16777216, actual=16777216
14/02/24 17:48:39 INFO namenode.FSNamesystem: fsOwner=bart4128
14/02/24 17:48:39 INFO namenode.FSNamesystem: supergroup=supergroup
14/02/24 17:48:39 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/02/24 17:48:39 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/02/24 17:48:39 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/02/24 17:48:39 INFO namenode.FSEditLog: dfs.namenode.edits.toleration.length = 0
14/02/24 17:48:39 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/02/24 17:48:40 INFO common.Storage: Image file /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/name/current/fsimage of size 114 bytes saved in 0 seconds.
14/02/24 17:48:40 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/name/current/edits
14/02/24 17:48:40 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/name/current/edits
14/02/24 17:48:40 INFO common.Storage: Storage directory /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/name has been successfully formatted.
14/02/24 17:48:40 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at node011/172.25.1.11
************************************************************/
Starting cluster - namenode/datanode: node011  slaves: node042
HADOOP_LOG_DIR = /g/home/bart4128/.hadoop/logs
starting namenode, logging to /g/home/bart4128/.hadoop/logs/hadoop-bart4128-namenode-node011.out
node042: starting datanode, logging to /g/home/bart4128/.hadoop/logs/hadoop-bart4128-datanode-node042.out
node042: starting secondarynamenode, logging to /g/home/bart4128/.hadoop/logs/hadoop-bart4128-secondarynamenode-node042.out
starting jobtracker, logging to /g/home/bart4128/.hadoop/logs/hadoop-bart4128-jobtracker-node011.out
node042: starting tasktracker, logging to /g/home/bart4128/.hadoop/logs/hadoop-bart4128-tasktracker-node042.out
##### Copying files to hdfs://node011:9000/ (/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu)
##### Running MapReduce examples
The current directory is /g/home/bart4128/Hadoop_playground/C_WordCount
/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu
##### Generating MinMax inputs
##### Running C Min Max
packageJobJar: [map.out, reduce.out, /tmp/hadoop-bart4128/hadoop-unjar2033054681156336982/] [] /tmp/streamjob6235402451898745886.jar tmpDir=null
14/02/24 17:49:18 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/02/24 17:49:18 WARN snappy.LoadSnappy: Snappy native library not loaded
14/02/24 17:49:18 INFO mapred.FileInputFormat: Total input paths to process : 8
14/02/24 17:49:19 INFO streaming.StreamJob: getLocalDirs(): [/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/local]
14/02/24 17:49:19 INFO streaming.StreamJob: Running job: job_201402241749_0001
14/02/24 17:49:19 INFO streaming.StreamJob: To kill this job, run:
14/02/24 17:49:19 INFO streaming.StreamJob: /g/software/hadoop/1.2.1/libexec/../bin/hadoop job  -Dmapred.job.tracker=node011:9001 -kill job_201402241749_0001
14/02/24 17:49:19 INFO streaming.StreamJob: Tracking URL: http://node011:50030/jobdetails.jsp?jobid=job_201402241749_0001
14/02/24 17:49:20 INFO streaming.StreamJob:  map 0%  reduce 0%
14/02/24 17:49:24 INFO streaming.StreamJob:  map 50%  reduce 0%
14/02/24 17:49:25 INFO streaming.StreamJob:  map 100%  reduce 0%
14/02/24 17:49:31 INFO streaming.StreamJob:  map 100%  reduce 33%
14/02/24 17:49:34 INFO streaming.StreamJob:  map 100%  reduce 100%
14/02/24 17:49:35 INFO streaming.StreamJob: Job complete: job_201402241749_0001
14/02/24 17:49:35 INFO streaming.StreamJob: Output: /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/
##### Finished running MapReduce job
##### Files on hdfs://node011:9000/
-rw-r--r--   2 bart4128 supergroup       5345 2014-02-24 17:49 /1121450.login006.sc.cluster.tamu.SC
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /tmp
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /tmp/hadoop-bart4128
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /tmp/hadoop-bart4128/mapred
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /tmp/hadoop-bart4128/mapred/staging
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /tmp/hadoop-bart4128/mapred/staging/bart4128
drwx------   - bart4128 supergroup          0 2014-02-24 17:49 /tmp/hadoop-bart4128/mapred/staging/bart4128/.staging
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs
-rw-r--r--   2 bart4128 supergroup     384440 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/file01.txt
-rw-r--r--   2 bart4128 supergroup    1916109 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/file02.txt
-rw-r--r--   2 bart4128 supergroup     662002 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/file03.txt
-rw-r--r--   2 bart4128 supergroup     581889 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/file04.txt
-rw-r--r--   2 bart4128 supergroup    1540023 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/file05.txt
-rw-r--r--   2 bart4128 supergroup       6448 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/temp.txt
-rw-r--r--   2 bart4128 supergroup        768 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/temp2.txt
-rw-r--r--   2 bart4128 supergroup       3224 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/inputs/temp3.txt
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs
-rw-r--r--   2 bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/_SUCCESS
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/_logs
drwxr-xr-x   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/_logs/history
-rw-r--r--   2 bart4128 supergroup      34262 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/_logs/history/job_201402241749_0001_1393285759297_bart4128_streamjob6235402451898745886.jar
-rw-r--r--   2 bart4128 supergroup      58500 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/_logs/history/job_201402241749_0001_conf.xml
-rw-r--r--   2 bart4128 supergroup     517692 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/outputs/part-00000
drwx------   - bart4128 supergroup          0 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/system
-rw-------   2 bart4128 supergroup          4 2014-02-24 17:49 /work/hadoop-bart4128-1121450.login006.sc.cluster.tamu/system/jobtracker.info
##### Copying files from hdfs://node011:9000/ (/work/hadoop-bart4128-1121450.login006.sc.cluster.tamu) to /g/home/bart4128/Hadoop_playground/C_WordCount
get: Target /g/home/bart4128/Hadoop_playground/C_WordCount/outputs/outputs/_SUCCESS already exists
##### Stop Cluster
stopping jobtracker
node042: stopping tasktracker
stopping namenode
node042: stopping datanode
node042: stopping secondarynamenode
##### END RUNNING JOB ##################################################
